{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMI9W9VAndXCY2oRsEvcYwM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaisshnavee1410/ASSIGNMENT-7-Logistic-Regression-.ipynb/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOGISTIC REGRESSION**"
      ],
      "metadata": {
        "id": "12BmxFEqYTDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.Data Exploration:**"
      ],
      "metadata": {
        "id": "VG0I_xDjZO22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a) Load the dataset and perform exploratory data analysis (EDA)**"
      ],
      "metadata": {
        "id": "WeiparXCavS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"Titanic_train.csv\")\n",
        "test_df = pd.read_csv(\"Titanic_test.csv\")\n"
      ],
      "metadata": {
        "id": "bpQ1lDXabYrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Examine the features, their types, and summary statistics.**"
      ],
      "metadata": {
        "id": "N2pTcFvlc9a9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d1_G6CBCUxu6"
      },
      "outputs": [],
      "source": [
        "# Display dataset information (feature names, data types, and non-null counts)\n",
        "print(\"Dataset Information:\")\n",
        "print(train_df.info())\n",
        "\n",
        "# Display summary statistics for numerical features\n",
        "print(\"\\nSummary Statistics for Numerical Features:\")\n",
        "print(train_df.describe())\n",
        "\n",
        "# Display summary statistics for categorical features\n",
        "print(\"\\nSummary Statistics for Categorical Features:\")\n",
        "print(train_df.describe(include=['O']))  # 'O' refers to object (categorical) data type\n",
        "\n",
        "# Display basic info and first few rows\n",
        "print(\"Dataset Information:\")\n",
        "print(train_df.info())\n",
        "print(\"\\nFirst 5 Rows of Training Data:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nSummary Statistics:\")\n",
        "print(train_df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) Create visualizations such as histograms, box plots, or pair plots to visualize the distributions and\n",
        "relationships between features.**"
      ],
      "metadata": {
        "id": "CXf5NOateSEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizations\n",
        "\n",
        "# Histogram of numerical features\n",
        "train_df.hist(figsize=(10, 8), bins=20, edgecolor='black')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box plot of numerical features\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(data=train_df[[\"Age\", \"Fare\", \"SibSp\", \"Parch\"]])\n",
        "plt.title(\"Box Plots of Numerical Features\")\n",
        "plt.show()\n",
        "\n",
        "# Correlation heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Select only numerical features for correlation calculation\n",
        "numerical_df = train_df.select_dtypes(include=['number'])\n",
        "sns.heatmap(numerical_df.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Pair plot\n",
        "train_df[\"Survived\"] = train_df[\"Survived\"].astype(\"category\")\n",
        "sns.pairplot(train_df, hue=\"Survived\", vars=[\"Age\", \"Fare\", \"SibSp\", \"Parch\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ot_w74F4fH1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**d) Analyze any patterns or correlations observed in the data.**"
      ],
      "metadata": {
        "id": "s0nqh9GxgDds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert categorical variables for visualization\n",
        "train_df[\"Survived\"] = train_df[\"Survived\"].astype(\"category\")\n",
        "train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\n",
        "train_df[\"Embarked\"] = train_df[\"Embarked\"].astype(\"category\")\n",
        "\n",
        "# 1. Survival rate by Passenger Class (Pclass)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df, ci=None, palette=\"coolwarm\")\n",
        "plt.title(\"Survival Rate by Passenger Class\")\n",
        "plt.ylabel(\"Survival Probability\")\n",
        "plt.xlabel(\"Passenger Class\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Survival rate by Gender (Sex)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.barplot(x=\"Sex\", y=\"Survived\", data=train_df, ci=None, palette=\"coolwarm\")\n",
        "plt.title(\"Survival Rate by Gender\")\n",
        "plt.ylabel(\"Survival Probability\")\n",
        "plt.xlabel(\"Gender\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Survival rate by Embarkation Port (Embarked)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.barplot(x=\"Embarked\", y=\"Survived\", data=train_df, ci=None, palette=\"coolwarm\")\n",
        "plt.title(\"Survival Rate by Embarkation Port\")\n",
        "plt.ylabel(\"Survival Probability\")\n",
        "plt.xlabel(\"Port of Embarkation\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Age distribution of survivors vs non-survivors\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.kdeplot(train_df.loc[train_df[\"Survived\"] == 1, \"Age\"], label=\"Survived\", shade=True, color=\"green\")\n",
        "sns.kdeplot(train_df.loc[train_df[\"Survived\"] == 0, \"Age\"], label=\"Did Not Survive\", shade=True, color=\"red\")\n",
        "plt.title(\"Age Distribution: Survivors vs Non-Survivors\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 5. Fare distribution for Survivors vs Non-Survivors\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x=\"Survived\", y=\"Fare\", data=train_df, palette=\"coolwarm\")\n",
        "plt.title(\"Fare Distribution by Survival Status\")\n",
        "plt.xlabel(\"Survived\")\n",
        "plt.ylabel(\"Fare\")\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iJ-j8rtQgVma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Data Preprocessing:**\n"
      ],
      "metadata": {
        "id": "devuZpc3kZrJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a)  Handle missing values (e.g., imputation).**"
      ],
      "metadata": {
        "id": "MBJcDsElkZnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"Titanic_train.csv\")\n",
        "test_df = pd.read_csv(\"Titanic_test.csv\")\n",
        "\n",
        "# =======================\n",
        "# 1. Checking Missing Values\n",
        "# =======================\n",
        "print(\"Missing values before handling:\\n\", train_df.isnull().sum())\n",
        "\n",
        "# =======================\n",
        "# 2. Handling Missing Values\n",
        "# =======================\n",
        "\n",
        "# Fill missing 'Age' values with the median age\n",
        "train_df[\"Age\"].fillna(train_df[\"Age\"].median(), inplace=True)\n",
        "test_df[\"Age\"].fillna(test_df[\"Age\"].median(), inplace=True)\n",
        "\n",
        "# Fill missing 'Fare' values with the median fare\n",
        "train_df[\"Fare\"].fillna(train_df[\"Fare\"].median(), inplace=True)\n",
        "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
        "\n",
        "# Fill missing 'Embarked' values with the mode (most common value)\n",
        "train_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0], inplace=True)\n",
        "test_df[\"Embarked\"].fillna(test_df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Drop 'Cabin' column (too many missing values)\n",
        "train_df.drop(columns=[\"Cabin\"], inplace=True)\n",
        "test_df.drop(columns=[\"Cabin\"], inplace=True)\n",
        "\n",
        "# =======================\n",
        "# 3. Checking Missing Values After Handling\n",
        "# =======================\n",
        "print(\"\\nMissing values after handling:\\n\", train_df.isnull().sum())"
      ],
      "metadata": {
        "id": "77CooYjTWlHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b) Encode categorical variables.**"
      ],
      "metadata": {
        "id": "ytVfXkjkky4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Encoding Categorical Variables\n",
        "\n",
        "# Convert 'Sex' column into numerical (0 = male, 1 = female)\n",
        "train_df[\"Sex\"] = train_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "test_df[\"Sex\"] = test_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "\n",
        "\n",
        "# 2. Display Processed Data\n",
        "\n",
        "print(\"Missing values after preprocessing:\\n\", train_df.isnull().sum())\n",
        "print(\"\\nFirst 5 rows of the processed training dataset:\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S83-Arjfk-No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Model Building:**"
      ],
      "metadata": {
        "id": "09pm0qLRR_dB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a). Build a logistic regression model using appropriate libraries (e.g., scikit-learn).**"
      ],
      "metadata": {
        "id": "mGo4Pr_ZSqzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"Titanic_train.csv\")\n",
        "test_df = pd.read_csv(\"Titanic_test.csv\")\n",
        "\n",
        "# Handle missing values\n",
        "train_df[\"Age\"].fillna(train_df[\"Age\"].median(), inplace=True)\n",
        "test_df[\"Age\"].fillna(test_df[\"Age\"].median(), inplace=True)\n",
        "\n",
        "train_df[\"Fare\"].fillna(train_df[\"Fare\"].median(), inplace=True)\n",
        "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
        "\n",
        "train_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0], inplace=True)\n",
        "test_df[\"Embarked\"].fillna(test_df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Drop 'Cabin' column (too many missing values)\n",
        "train_df.drop(columns=[\"Cabin\"], inplace=True)\n",
        "test_df.drop(columns=[\"Cabin\"], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "train_df[\"Sex\"] = train_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "test_df[\"Sex\"] = test_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "\n",
        "train_df[\"Embarked\"] = train_df[\"Embarked\"].map({\"C\": 0, \"Q\": 1, \"S\": 2})\n",
        "test_df[\"Embarked\"] = test_df[\"Embarked\"].map({\"C\": 0, \"Q\": 1, \"S\": 2})\n",
        "\n",
        "# ==========================\n",
        "# Select Features and Target\n",
        "# ==========================\n",
        "# Select relevant features\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]\n",
        "X = train_df[features]  # Independent variables\n",
        "y = train_df[\"Survived\"]  # Target variable\n",
        "\n",
        "# Split data into training and validation sets (80% train, 20% test)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (improves model performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# ==========================\n",
        "# Train Logistic Regression Model\n",
        "# ==========================\n",
        "# Initialize and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ==========================\n",
        "# Model Evaluation\n",
        "# ==========================\n",
        "# Predict on validation data\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Display confusion matrix\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_pred))"
      ],
      "metadata": {
        "id": "6qzpKx4nURxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**b). Train the model using the training data.**"
      ],
      "metadata": {
        "id": "UQ-KvJrWXVhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"Titanic_train.csv\")\n",
        "test_df = pd.read_csv(\"Titanic_test.csv\")\n",
        "\n",
        "\n",
        "# Handle missing values\n",
        "train_df[\"Age\"].fillna(train_df[\"Age\"].median(), inplace=True)\n",
        "train_df[\"Fare\"].fillna(train_df[\"Fare\"].median(), inplace=True)\n",
        "train_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "train_df[\"Sex\"] = train_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "train_df[\"Embarked\"] = train_df[\"Embarked\"].map({\"C\": 0, \"Q\": 1, \"S\": 2})\n",
        "\n",
        "# ==========================\n",
        "# Select Features and Target\n",
        "# ==========================\n",
        "features = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]\n",
        "X = train_df[features]  # Independent variables\n",
        "y = train_df[\"Survived\"]  # Target variable\n",
        "\n",
        "# Split data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features (scaling helps improve model performance)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# ==========================\n",
        "# Train the Logistic Regression Model\n",
        "# ==========================\n",
        "# Initialize the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train (fit) the model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ==========================\n",
        "# Model Training Complete\n",
        "# ==========================\n",
        "print(\"Logistic Regression model trained successfully!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "G3r7V0AEVyjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.Model Evaluation:**"
      ],
      "metadata": {
        "id": "GQnfT2nBYSla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a). Evaluate the performance of the model on the testing data using accuracy, precision, recall, F1-\n",
        "score, and ROC-AUC score.**\n",
        "\n",
        "  \n",
        "  **Visualize the ROC curve.**"
      ],
      "metadata": {
        "id": "iTz4wlx_Yu3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"Titanic_train.csv\")\n",
        "test_df = pd.read_csv(\"Titanic_test.csv\")\n",
        "\n",
        "# Assuming X_val and y_val are your validation data\n",
        "y_pred = model.predict(X_val)  # Predictions on validation data\n",
        "y_pred_proba = model.predict_proba(X_val)[:, 1]  # Probability of survival\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "precision = precision_score(y_val, y_pred)\n",
        "recall = recall_score(y_val, y_pred)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_pred))\n",
        "\n",
        "# ==========================\n",
        "# 5. Plot the ROC Curve\n",
        "# ==========================\n",
        "fpr, tpr, _ = roc_curve(y_val, y_pred_proba)  # Use y_val here\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color=\"blue\", label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Diagonal line\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve for Titanic Survival Prediction\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1gK-v3xqpdCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Interpretation:**"
      ],
      "metadata": {
        "id": "1L0RFZ-IaPHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a). Interpret the coefficients of the logistic regression model.**"
      ],
      "metadata": {
        "id": "6IEcsERIasgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv(\"Titanic_train.csv\")\n",
        "test_df = pd.read_csv(\"Titanic_test.csv\")\n",
        "\n",
        "# Interpret Model Coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame to display feature names with their respective coefficients\n",
        "coef_df = pd.DataFrame({\"Feature\": features, \"Coefficient\": coefficients})\n",
        "\n",
        "# Sort by absolute coefficient values\n",
        "coef_df[\"Abs_Coefficient\"] = coef_df[\"Coefficient\"].abs()\n",
        "coef_df = coef_df.sort_values(by=\"Abs_Coefficient\", ascending=False).drop(columns=[\"Abs_Coefficient\"])\n",
        "\n",
        "# Display the coefficients\n",
        "print(\"\\nLogistic Regression Coefficients:\\n\")\n",
        "print(coef_df)"
      ],
      "metadata": {
        "id": "_igvCsA7aXkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b). Discuss the significance of features in predicting the target variable (survival probability in this\n",
        "case)**"
      ],
      "metadata": {
        "id": "vBQ2gMila0XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Titanic dataset\n",
        "train_df = pd.read_csv(\"Titanic_train.csv\")\n",
        "test_df = pd.read_csv(\"Titanic_test.csv\")\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    # Encode categorical variables\n",
        "    df['Sex'] = LabelEncoder().fit_transform(df['Sex'])  # Male=1, Female=0\n",
        "    df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "    df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])\n",
        "\n",
        "    # Fill missing values in Age using median\n",
        "    imputer = SimpleImputer(strategy=\"median\")\n",
        "    df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df = preprocess_data(train_df)\n",
        "test_df = preprocess_data(test_df)\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "X = train_df[features]\n",
        "y = train_df['Survived']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Cross-validation score\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)\n",
        "print(\"\\nCross-validation Accuracy:\", np.mean(cv_scores))\n",
        "\n",
        "# Feature Importance Visualization\n",
        "feature_importance = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=feature_importance, y=feature_importance.index, palette=\"viridis\")\n",
        "plt.xlabel(\"Feature Importance Score\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature Importance for Survival Prediction\")\n",
        "plt.show()\n",
        "\n",
        "# Exploratory Data Analysis (EDA) - Survival Rate by Gender\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=train_df[\"Sex\"], y=train_df[\"Survived\"], palette=\"coolwarm\")\n",
        "plt.xticks([0, 1], [\"Female\", \"Male\"])\n",
        "plt.ylabel(\"Survival Rate\")\n",
        "plt.title(\"Survival Rate by Gender\")\n",
        "plt.show()\n",
        "\n",
        "# Predict on test dataset\n",
        "test_predictions = model.predict(test_df[features])\n",
        "\n",
        "# Save predictions\n",
        "submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': test_predictions})\n",
        "submission.to_csv(\"Titanic_Survival_Predictions.csv\", index=False)\n",
        "print(\"Predictions saved to Titanic_Survival_Predictions.csv\")"
      ],
      "metadata": {
        "id": "q5mXccr6ruWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **INTERVIEW QUESTIONS:**"
      ],
      "metadata": {
        "id": "bWDXb7GlgY_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is the difference between precision and recall?**"
      ],
      "metadata": {
        "id": "O2Orrunphn85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Precision and recall are key metrics used in evaluating the performance of classification models, especially in imbalanced datasets.\n",
        "\n",
        "* **Precision (Positive Predictive Value):**\n",
        "\n",
        "  Precision measures how many of the predicted positive cases are actually positive. It focuses on the correctness of positive predictions.\n",
        "\n",
        "\n",
        " **Example:** If a spam filter detects 100 spam emails but only 80 of them are actual spam (and 20 are not), the precision is \\frac{80}{100} = 0.8 (80%).\n",
        "\n",
        "\n",
        "* **Recall (Sensitivity or True Positive Rate):**\n",
        "\n",
        "  Recall measures how many actual positive cases were correctly identified by the model. It focuses on minimizing false negatives.\n",
        "\n",
        "**Example:** If there are 120 spam emails in total, and the model correctly identifies 80 of them as spam, the recall is \\frac{80}{120} = 0.67 (67%).\n",
        "\n",
        "* **Key Difference:**\n",
        "\n",
        "•\tPrecision is about correctness of positive predictions.\n",
        "\n",
        "•\tRecall is about completeness in finding positive cases.\n",
        "\n",
        "* **Trade-off Between Precision and Recall:**\n",
        "\n",
        "•\tIncreasing precision often decreases recall (fewer false positives but more false negatives).\n",
        "\n",
        "•\tIncreasing recall often decreases precision (fewer false negatives but more false positives)."
      ],
      "metadata": {
        "id": "-jvss9LMiL3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. What is cross-validation, and why is it important in binary classification?**"
      ],
      "metadata": {
        "id": "0TQo6keghrQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation is a technique used in machine learning to evaluate the performance of a model by splitting the dataset into multiple subsets. It helps ensure that the model generalizes well to unseen data rather than just memorizing patterns in the training data.\n",
        "\n",
        "* **How Cross-Validation Works:**\n",
        "\n",
        "1.\tThe dataset is divided into k subsets (or “folds”).\n",
        "\n",
        "2.\tThe model is trained on k-1 folds and tested on the remaining fold.\n",
        "\n",
        "3.\tThis process repeats k times, each time using a different fold as the test set.\n",
        "\n",
        "4.\tThe final performance metric (e.g., accuracy, precision, recall) is averaged across all k iterations.\n",
        "\n",
        "\n",
        "**Example:** (5-Fold Cross-Validation):\n",
        "\n",
        "1.\tSplit data into 5 folds.\n",
        "\n",
        "2.\tTrain on 4 folds, test on 1 fold.\n",
        "\n",
        "3.\tRepeat this 5 times, each time selecting a different fold for testing.\n",
        "\n",
        "4.\tAverage the results.\n",
        "\n",
        "* **Why is Cross-Validation Important in Binary Classification?**\n",
        "\n",
        "Binary classification involves predicting one of two classes (e.g., spam vs. not spam, disease vs. no disease). Cross-validation is especially useful in this case because:\n",
        "\n",
        "  **1.\tPrevents Overfitting:**\n",
        "\n",
        "•\tEnsures the model is not just memorizing patterns but generalizing well to new data.\n",
        "\n",
        "**2.\tProvides More Reliable Metrics:**\n",
        "\n",
        "•\tInstead of relying on a single train-test split, cross-validation averages performance over multiple trials.\n",
        "\n",
        "**3.\tHandles Imbalanced Datasets Better:**\n",
        "\n",
        "•\tIn binary classification with class imbalance, a simple train-test split might not capture the minority class well. Cross-validation ensures all data is used for training and testing.\n",
        "\n",
        "**4.\tOptimizes Hyperparameters:**\n",
        "\n",
        "•\tWhen tuning hyperparameters (e.g., learning rate, depth of a decision tree), cross-validation provides a better estimate of the best settings.\n",
        "\n",
        "**5.\tMakes Efficient Use of Data:**\n",
        "\n",
        "•\tInstead of wasting data by setting aside a large test set, cross-validation ensures every data point is used for both training and testing at some point.\n",
        "\n",
        "\n",
        "**Types of Cross-Validation**\n",
        "\n",
        "1.\tk-Fold Cross-Validation (Most Common)\n",
        "\t•\tData is split into k parts, and training/testing happens k times.\n",
        "\n",
        "2.\tStratified k-Fold Cross-Validation\n",
        "\t•\tEnsures each fold has the same proportion of classes as the original dataset (important for imbalanced datasets).\n",
        "\n",
        "3.\tLeave-One-Out Cross-Validation (LOOCV)\n",
        "\t•\tEach data point is used as a test set once, and all other points are used for training.\n",
        "\n",
        "* **Conclusion:**\n",
        "\n",
        "Cross-validation is a powerful technique that improves the reliability and generalizability of a binary classification model. It ensures the model is evaluated fairly, prevents overfitting, and helps in hyperparameter tuning."
      ],
      "metadata": {
        "id": "3OuFD0-JjsHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Odd_VtAjh1m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}